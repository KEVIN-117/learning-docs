podrimos definir a los `Embeddings` como la representaciÃ³n vectorial de los datos con los cuales trabajan los LLM

uno de los conceptos que se suelen utilizar mucho es:
## Similitud por coseno:
### ðŸ”¹Â¿CÃ³mo funciona la similitud por coseno?

Dado dos vectores **A** y **B**, la similitud por coseno se define como:

$$
{Similitud}(A, B) = \frac{A \cdot B}{\|A\| \|B\|}
$$
Donde:

- $Aâ‹…B$ es el **producto punto** de los vectores.
- $âˆ¥Aâˆ¥\|B\|$ son las **normas** (o magnitudes) de los vectores.

El resultado es un valor entre **-1 y 1**, donde:

- **1** significa que los vectores son **idÃ©nticos** (mÃ¡xima similitud).
- **0** significa que los vectores son **ortogonales** (no tienen relaciÃ³n).
- **-1** significa que los vectores son **opuestos** (mÃ­nima similitud).

#### ðŸ”¹ **Ejemplo en embeddings**

En NLP (procesamiento de lenguaje natural), los embeddings convierten palabras en vectores de alta dimensiÃ³n. La similitud por coseno nos permite medir quÃ© tan parecidas son dos palabras.

Por ejemplo, si usamos un modelo de embeddings como Word2Vec o BERT:

- **Similitud("gato", "perro") â‰ˆ 0.8** â†’ Son similares.
- **Similitud("gato", "computadora") â‰ˆ 0.2** â†’ Son poco similares.
- **Similitud("gato", "agua") â‰ˆ 0.0** â†’ No tienen relaciÃ³n.

#### ðŸ”¹ **Â¿Por quÃ© usar similitud por coseno?**

A diferencia de la distancia euclidiana, la similitud por coseno **ignora la magnitud** de los vectores y solo se enfoca en la **direcciÃ³n**, lo que es Ãºtil cuando los datos estÃ¡n en diferentes escalas.

- ðŸ”¹ **Aplicaciones**:
	- **BÃºsqueda de informaciÃ³n** (documentos similares).
	- **Sistemas de recomendaciÃ³n**.
	- **Agrupamiento de datos (clustering)**.
	- **Reconocimiento facial (comparando embeddings de imÃ¡genes)**.

# Â¿CÃ³mo funcionan los embeddings?
## IntroducciÃ³n a One-Hot Encoding y TF-IDF en IA
Estas son tÃ©cnicas utilizadas para el procesamiento de datos en inteligencia artificial $(AI)$, especialmente en los modelos de procesamiento de lenguaje natural ($NLP$) 
### One-Hot Encoding
Este mÃ©todo ayuda a clasificar el texto palabra por palabra para mapearlos, la longitud de la fila de la matriz se determina por la cantidad de palabras que hay en el texto, este mÃ©todo se usa en textos muy cortos.
TÃ©cnica usada para clasificar variables categÃ³ricas (palabras, etiquetas, categorÃ­as), esto consiste en convertir cada `palabra` en una vector binario, es decir en un vector que solo tiene `0` `inactivas` o `1` `activos` donde solo una posiciÃ³n esta activa en cada vector.
#### Â¿CÃ³mo funciona?

- Supongamos que tienes una lista de categorÃ­as:Â `["gato", "perro", "pÃ¡jaro"]`.
    
- Aplicando One-Hot Encoding, cada categorÃ­a se convierte en un vector de longitud igual al nÃºmero de categorÃ­as Ãºnicas:
    
    - `"gato"`Â â†’Â `[1, 0, 0]`
        
    - `"perro"`Â â†’Â `[0, 1, 0]`
        
    - `"pÃ¡jaro"`Â â†’Â `[0, 0, 1]`
#### Ejemplo en NLP:

Si tienes una oraciÃ³n:Â `"El gato y el perro"`, y tu vocabulario esÂ `["el", "gato", "y", "perro"]`, el One-Hot Encoding serÃ­a:

- `"el"`Â â†’Â `[1, 0, 0, 0]`
    
- `"gato"`Â â†’Â `[0, 1, 0, 0]`
    
- `"y"`Â â†’Â `[0, 0, 1, 0]`
    
- `"perro"`Â â†’Â `[0, 0, 0, 1]`
    

#### Ventajas:

- Simple y fÃ¡cil de implementar.
    
- Ãštil cuando las categorÃ­as no tienen una relaciÃ³n ordinal (no hay un orden inherente).
    

#### Desventajas:

- Puede generar vectores muy dispersos (con muchos 0s) si el vocabulario es grande.
    
- No captura relaciones semÃ¡nticas entre palabras (por ejemplo, "gato" y "perro" no estÃ¡n relacionadas en el espacio vectorial).

Â¡Claro! Vamos a explicar ambos conceptos, **One-Hot Encoding** y **TF-IDF**, que son tÃ©cnicas comÃºnmente utilizadas en el procesamiento de datos para inteligencia artificial (IA), especialmente en el campo del procesamiento de lenguaje natural (NLP).

---

### **1. One-Hot Encoding:**

#### Â¿QuÃ© es?
El **One-Hot Encoding** es una tÃ©cnica utilizada para representar variables categÃ³ricas (como palabras, etiquetas o categorÃ­as) en un formato numÃ©rico que los modelos de IA puedan entender. Consiste en convertir cada categorÃ­a en un vector binario (compuesto de 0s y 1s), donde solo una posiciÃ³n del vector es 1 (activa) y el resto son 0s (inactivas).

#### Â¿CÃ³mo funciona?
- Supongamos que tienes una lista de categorÃ­as: `["gato", "perro", "pÃ¡jaro"]`.
- Aplicando One-Hot Encoding, cada categorÃ­a se convierte en un vector de longitud igual al nÃºmero de categorÃ­as Ãºnicas:

  - `"gato"` â†’ `[1, 0, 0]`
  - `"perro"` â†’ `[0, 1, 0]`
  - `"pÃ¡jaro"` â†’ `[0, 0, 1]`

#### Ejemplo en NLP:
Si tienes una oraciÃ³n: `"El gato y el perro"`, y tu vocabulario es `["el", "gato", "y", "perro"]`, el One-Hot Encoding serÃ­a:

- `"el"` â†’ `[1, 0, 0, 0]`
- `"gato"` â†’ `[0, 1, 0, 0]`
- `"y"` â†’ `[0, 0, 1, 0]`
- `"perro"` â†’ `[0, 0, 0, 1]`

---

### **2. TF-IDF (Term Frequency-Inverse Document Frequency):**

#### Â¿QuÃ© es?
El **TF-IDF** es una tÃ©cnica utilizada para cuantificar la importancia de una palabra en un documento dentro de un conjunto de documentos (corpus). Es especialmente Ãºtil en tareas de recuperaciÃ³n de informaciÃ³n y minerÃ­a de texto.

#### Â¿CÃ³mo funciona?
TF-IDF combina dos mÃ©tricas:
 - **TF (Term Frequency)**: Mide la frecuencia de una palabra en un documento.
   $$
   {TF}(t, d) = \frac{\text{NÃºmero de veces que aparece la palabra } t \text{ en el documento } d}{\text{NÃºmero total de palabras en el documento } d}
$$
- **IDF (Inverse Document Frequency)**: Mide la importancia de una palabra en el corpus. Las palabras que aparecen en muchos documentos tienen un IDF bajo.
   $$
   {IDF}(t, D) = \log\left(\frac{\text{NÃºmero total de documentos en el corpus } D}{\text{NÃºmero de documentos que contienen la palabra } t}\right)
   $$
- **TF-IDF**: Es el producto de TF e IDF.
   $$
   \text{TF-IDF}(t, d, D) = \text{TF}(t, d) \cdot \text{IDF}(t, D)
   $$

#### Ejemplo:
Supongamos que tienes un corpus con dos documentos:
- Documento 1: `"El gato come pescado"`
- Documento 2: `"El perro come carne"`

Calculamos el TF-IDF para la palabra `"come"` en el Documento 1:
1. **TF**: La palabra `"come"` aparece 1 vez en el Documento 1, que tiene 4 palabras.
   $$
   \text{TF} = \frac{1}{4} = 0.25
   $$
2. **IDF**: La palabra `"come"` aparece en 2 documentos (Documento 1 y Documento 2) de un total de 2 documentos.
   $$
   \text{IDF} = \log\left(\frac{2}{2}\right) = \log(1) = 0
   $$
3. **TF-IDF**:
   $$
   \text{TF-IDF} = 0.25 \cdot 0 = 0
   $$

---
### **ComparaciÃ³n entre One-Hot Encoding y TF-IDF:**

| CaracterÃ­stica        | One-Hot Encoding            | TF-IDF                                 |
| --------------------- | --------------------------- | -------------------------------------- |
| **RepresentaciÃ³n**    | Vectores binarios (0s y 1s) | Vectores numÃ©ricos (valores continuos) |
| **Uso comÃºn**         | Variables categÃ³ricas       | Importancia de palabras en documentos  |
| **Captura semÃ¡ntica** | No                          | No                                     |
| **DispersiÃ³n**        | Muy disperso (muchos 0s)    | Menos disperso                         |
| **Ejemplo de uso**    | ClasificaciÃ³n de categorÃ­as | RecuperaciÃ³n de informaciÃ³n, NLP       |

### **Resumen:**

- **One-Hot Encoding**: Representa categorÃ­as como vectores binarios. Es simple pero no captura relaciones semÃ¡nticas.
    
- **TF-IDF**: Cuantifica la importancia de una palabra en un documento relativo a un corpus. Es Ãºtil para tareas de NLP pero tampoco captura semÃ¡ntica.
## RepresentaciÃ³n Vectorial de Palabras


## EvaluaciÃ³n de Similitudes SemÃ¡nticas: MÃ©todos y Aplicaciones

# CreaciÃ³n de embeddings

## CreaciÃ³n y entrenamiento de modelos Word2Vec con Gensim
